{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "from pytextprocess import *\n",
    "from dataRetrieval import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from my_latent_dirichlet_allocation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('backup/jokes.pickle', \"rb\") as f:\n",
    "        jokes = pickle.load(f)\n",
    "    f.close()\n",
    "    with open('backup/preprocessed_jokes.pickle', \"rb\") as f:\n",
    "        preprocessed_jokes = pickle.load(f)\n",
    "    f.close()\n",
    "except IOError as e:\n",
    "    print (\"I/O error: {0}\".format(e))\n",
    "    preprocesser = Preprocesser(tagger_backup='backup/tagger.pickle')\n",
    "    jokes, preprocessed_jokes = preprocesser.preprocess()\n",
    "    with open('backup/jokes.pickle', 'wb') as f:\n",
    "        pickle.dump(jokes, f)\n",
    "    f.close()\n",
    "    with open('backup/preprocessed_jokes.pickle', 'wb') as f:\n",
    "        pickle.dump(preprocessed_jokes, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'data/jester_dataset_2/'\n",
    "with open(os.path.join(path, 'jester_items.dat')) as f:\n",
    "    jokes = f.read()\n",
    "f.close()\n",
    "html_jokes = jokes.split('</p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_jokes = [re.sub(r'\\r\\n\\n', '', joke) for joke in html_jokes]\n",
    "html_jokes = [re.sub(r'&quot;', '\\\"', joke) for joke in html_jokes]\n",
    "html_jokes = [re.sub('&#039;', \"\\'\", joke) for joke in html_jokes]\n",
    "html_jokes = [re.sub(r'\\n', '', joke) for joke in html_jokes]\n",
    "html_jokes = [re.sub(r'\\r', '', joke) for joke in html_jokes]\n",
    "html_jokes = [re.sub(r'<p>', '', joke) for joke in html_jokes]\n",
    "html_jokes = [re.sub(r'<br />', '', joke) for joke in html_jokes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extracter = Extracter()\n",
    "#tokenizer = Tokenizer()\n",
    "#tagger = Tagger()\n",
    "#replacer = AntonymReplacer()\n",
    "#lemmatizer = Lemmatizer()\n",
    "preprocessed_jokes = []\n",
    "for joke in html_jokes:\n",
    "    # Jokes tokenization...\n",
    "    tokenized_joke = tokenizer.tokenize(joke, remove_stopwords=False)\n",
    "    # Jokes tokenization done!\n",
    "    # Jokes POS Tagging...\n",
    "    tagged_sentences = tagger.tag_sentences(tokenized_joke)\n",
    "    tagged_joke = tagger.map_tag(tagged_sentences)\n",
    "    # Jokes POS Tagging done!\n",
    "    # Jokes negation replacement...\n",
    "    tagged_joke2 = []\n",
    "    for sentence in tagged_joke:\n",
    "        tagged_joke2.append(replacer.replace_negations(sentence))\n",
    "    # Jokes negation replacement done!\n",
    "    # Jokes lemmatization...\n",
    "    lemmatized_joke = lemmatizer.lemmatize_sentences(tagged_joke2)\n",
    "    # Jokes lemmatization done!\n",
    "    # Jokes removing stopwords and punctuation...\n",
    "    normalized_joke = extracter.remove_stopwords(lemmatized_joke)\n",
    "    # Jokes removing stopwords and punctuation!\n",
    "    # Jokes transformation from list of tokens into concatenation of tokens...\n",
    "    preprocessed_joke = ''\n",
    "    for normalized_sentence in normalized_joke:\n",
    "        preprocessed_joke = preprocessed_joke +' '+' '.join(normalized_sentence)\n",
    "    preprocessed_jokes.append(preprocessed_joke.strip())\n",
    "    # Jokes transformation from list of tokens into concatenation of tokens!\n",
    "preprocessed_jokes = preprocessed_jokes[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_jokes = preprocessed_jokes[:150]\n",
    "#preprocessed_jokes = [re.sub(r'.', ' ', joke) for joke in preprocessed_jokes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = CountVectorizer(token_pattern='[a-zA-Z]{3,}',max_df=0.95, min_df=0.0002,max_features=2000,stop_words='english')\n",
    "jokes_words = v.fit_transform(preprocessed_jokes)\n",
    "word_index = v.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20, max_iter=10, learning_method='online',\n",
    "                                       learning_offset=10., random_state=0, n_jobs=-1)\n",
    "lda.fit(jokes_words)\n",
    "jokes_topics = lda.transform(jokes_words)\n",
    "docs_topics_norm = jokes_topics/jokes_topics.sum(axis=1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "similarity = 1-pairwise_distances(docs_topics_norm, metric=\"cosine\")\n",
    "similarity = np.tril(similarity,k=-1)\n",
    "corre = np.tril(pairwise_distances(docs_topics_norm, metric='correlation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(49, 38), (101, 38), (69, 16), (49, 6), (38, 3), (93, 38), (61, 38), (49, 16), (117, 16), (87, 49)]\n",
      "1.15493556676\n",
      "1.13612312617\n",
      "1.12690535413\n",
      "1.12619195748\n",
      "1.12479763396\n",
      "1.12470478971\n",
      "1.12416542092\n",
      "1.12304642855\n",
      "1.12138010134\n",
      "1.121082644\n"
     ]
    }
   ],
   "source": [
    "c = corre.copy()\n",
    "\n",
    "index_max = []\n",
    "for i in range(10):\n",
    "    index_max.append(unravel_index(c.argmax(), c.shape))\n",
    "    c[index_max[i][0],index_max[i][1]] = 0\n",
    "\n",
    "print index_max\n",
    "\n",
    "for index in index_max:\n",
    "    print corre[index[0], index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70:Employer to applicant: \"In this job we need someone who is responsible.\"Applicant: \"I'm the one you want. On my last job, every time anything went wrong, they said I was responsible.\"\n",
      "17:How many men does it take to screw in a light bulb?One. Men will screw anything.\n"
     ]
    }
   ],
   "source": [
    "print html_jokes[69]\n",
    "print html_jokes[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, 19), (145, 134), (76, 40), (97, 91), (100, 98), (132, 1), (57, 9), (136, 55), (141, 90), (119, 62)]\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.999999992753\n",
      "0.999999991629\n",
      "0.999999989243\n"
     ]
    }
   ],
   "source": [
    "s = similarity.copy()\n",
    "\n",
    "index_max = []\n",
    "for i in range(10):\n",
    "    index_max.append(unravel_index(s.argmax(), s.shape))\n",
    "    s[index_max[i][0],index_max[i][1]] = 0\n",
    "\n",
    "print index_max\n",
    "\n",
    "for index in index_max:\n",
    "    print similarity[index[0], index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133:The new employee stood before the paper shredder looking confused. \"Need some help?\" a secretary, walking by, asked. \"Yes,\" he replied, \"how does this thing work?\" \"Simple,\" she said, taking the fat report from his hand and feeding it into the shredder.\"Thanks, but where do the copies come out?\"\n",
      "2:This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\"\"They told me that you were a pedophile.\"He replied, \"That's an awfully big word for a ten year old.\"\n"
     ]
    }
   ],
   "source": [
    "print html_jokes[19]\n",
    "print html_jokes[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "retriever = DataRetriever()\n",
    "data1 = retriever.read_from_csv_file('jester-1-data.csv')\n",
    "df1 = data1.drop(['#rating'], axis=1).copy()\n",
    "print 'Shape of df1 : {}'.format(df1.shape)\n",
    "print 'done in %0.3fs.' % (time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df1.replace(to_replace=99, value=np.nan)\n",
    "print 'Density: {}'.format(np.sum(df.count())/float(nb_user*nb_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparse = []\n",
    "for i, user in df1.iterrows():\n",
    "    for j, item in user.iteritems():\n",
    "        if item!=99:\n",
    "            sparse.append((i,j,item))\n",
    "sparse_df = pd.DataFrame(sparse, columns=['userid', 'jokeid','rating'])\n",
    "sparse_df['rating'] += 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need ~334sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retriever = DataRetriever()\n",
    "t0 = time()\n",
    "B, user_by_rowindex, movie_by_colindex = retriever.build_sparse_matrix(sparse_df, 'userid', 'jokeid', 'rating')\n",
    "num_users, num_movies = B.shape\n",
    "print 'done in %0.3fs.' % (time() - t0)\n",
    "print '{} joke, {} user'.format(num_movies, num_users)\n",
    "print 'Density: {}'.format(float(B.getnnz()) / (num_users * num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD,FactorAnalysis\n",
    "pca = TruncatedSVD(n_components=71)\n",
    "pca.fit(B)\n",
    "X_pca = pca.transform(B)\n",
    "B_dense = B.todense()\n",
    "C = pca.components_\n",
    "print 'Shape of PCA Components : {}'.format(pca.components_.shape)\n",
    "print 'Shape of X_pca : {}'.format(X_pca.shape)\n",
    "print 'Shape of B : {}'.format(B_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sum(pca.explained_variance_ratio_)\n",
    "reconstructed_B = X_pca.dot(C)\n",
    "reconstructed_B = pd.DataFrame(reconstructed_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation : from excel to dateframe (need ~85.271 sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "path = 'data/xls/'\n",
    "retriever = DataRetriever()\n",
    "\n",
    "table1 = retriever.read_from_excel_file('jester-data-1.xls')\n",
    "table2 = retriever.read_from_excel_file('jester-data-2.xls')\n",
    "table3 = retriever.read_from_excel_file('jester-data-3.xls')\n",
    "\n",
    "print 'Shape of table1 : {}'.format(table1.shape)\n",
    "print 'Shape of table2 : {}'.format(table2.shape)\n",
    "print 'Shape of table3 : {}'.format(table3.shape)\n",
    "\n",
    "frames12 = [table1, table2]\n",
    "df12 = pd.concat(frames12)\n",
    "print 'Shape of result : {}'.format(df12.shape)\n",
    "\n",
    "frames13 = [table1, table3]\n",
    "df13 = pd.concat(frames13)\n",
    "print 'Shape of result : {}'.format(df13.shape)\n",
    "\n",
    "frames23 = [table2, table3]\n",
    "df23 = pd.concat(frames23)\n",
    "print 'Shape of result : {}'.format(df23.shape)\n",
    "\n",
    "retriever.store_to_csv(df12, path_to_csv='data/csv/', name='jester-12-data.csv')\n",
    "retriever.store_to_csv(df13, path_to_csv='data/csv/', name='jester-13-data.csv')\n",
    "retriever.store_to_csv(df23, path_to_csv='data/csv/', name='jester-23-data.csv')\n",
    "retriever.store_to_csv(table1, path_to_csv='data/csv/', name='jester-1-data.csv')\n",
    "retriever.store_to_csv(table2, path_to_csv='data/csv/', name='jester-2-data.csv')\n",
    "retriever.store_to_csv(table3, path_to_csv='data/csv/', name='jester-3-data.csv')\n",
    "print 'done in %0.3fs.' % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataRetrieval import *\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "from pyfm import pylibfm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need about ~5 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thebatou/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:648: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators; you can avoid this warning by specifying engine='python'.\n",
      "  ParserWarning)\n"
     ]
    }
   ],
   "source": [
    "path = 'data/jester_dataset_2/'\n",
    "df = pd.read_csv(os.path.join(path, 'jester_ratings.dat'), sep='\\t\\t', names=['userid', 'jokeid','rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need 230 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 179.618s.\n",
      "140 joke, 59132 user\n",
      "Density: 0.212304674288\n"
     ]
    }
   ],
   "source": [
    "retriever = DataRetriever()\n",
    "t0 = time()\n",
    "B, user_by_rowindex, movie_by_colindex = retriever.build_sparse_matrix(df, 'userid', 'jokeid', 'rating')\n",
    "num_users, num_movies = B.shape\n",
    "print 'done in %0.3fs.' % (time() - t0)\n",
    "print '{} joke, {} user'.format(num_movies, num_users)\n",
    "print 'Density: {}'.format(float(B.getnnz()) / (num_users * num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD,FactorAnalysis\n",
    "pca = TruncatedSVD(n_components=71)\n",
    "pca.fit(B)\n",
    "X_pca = pca.transform(B)\n",
    "B_dense = B.todense()\n",
    "C = pca.components_\n",
    "print 'Shape of PCA Components : {}'.format(pca.components_.shape)\n",
    "print 'Shape of X_pca : {}'.format(X_pca.shape)\n",
    "print 'Shape of B : {}'.format(B_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(X_pca5, \"Principal Components projection\")\n",
    "plot_embedding(X_pca3O, \"Principal Components projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=2, algorithm='brute', metric='cosine', n_jobs=-1)\n",
    "neigh.fit(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import unravel_index\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_pearson_correlation(X):\n",
    "    n,m = X.shape\n",
    "    pearson_matrix = np.zeros((m,m))\n",
    "    j = 0\n",
    "    while j < m:\n",
    "        for i in range(j+1,m):\n",
    "            pearson_matrix[j,i] = pearsonr(X[:,j], X[:,i])[0]\n",
    "        j += 1\n",
    "    return pearson_matrix\n",
    "\n",
    "def compute_cosine_similarity(A):\n",
    "    similarity = A.dot(A.T)\n",
    "    square_mag = A.diag()\n",
    "    inv_square_mag = 1 / square_mag\n",
    "    inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
    "    inv_mag = np.sqrt(inv_square_mag)\n",
    "    cosine = similarity * inv_mag\n",
    "    return cosine.T * inv_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pearson = compute_pearson_correlation(X)\n",
    "#cosine = compute_cosine_similarity(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = pearson.copy()\n",
    "\n",
    "index_max = []\n",
    "for i in range(5):\n",
    "    index_max.append(unravel_index(p.argmax(), p.shape))\n",
    "    p[index_max[i][0],index_max[i][1]] = 0\n",
    "\n",
    "print index_max\n",
    "\n",
    "for index in index_max:\n",
    "    print pearson[index[0], index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "data1 = retriever.read_from_csv_file('jester-1-data.csv')\n",
    "#data2 = retriever.read_from_csv_file('jester-2-data.csv')\n",
    "#data3 = retriever.read_from_csv_file('jester-3-data.csv')\n",
    "#data12 = read_from_csv_file('jester-12-data.csv')\n",
    "#data13 = read_from_csv_file('jester-13-data.csv')\n",
    "#data23 = read_from_csv_file('jester-23-data.csv')\n",
    "\n",
    "df1 = data1.drop(['#rating'], axis=1).copy()\n",
    "print 'Shape of df1 : {}'.format(df1.shape)\n",
    "#df2 = data2.drop(['#rating'], axis=1).copy()\n",
    "#print 'Shape of df2 : {}'.format(df2.shape)\n",
    "#df3 = data3.drop(['#rating'], axis=1).copy()\n",
    "#print 'Shape of df3 : {}'.format(df3.shape)\n",
    "#df12 = data12.drop(['#rating'], axis=1).copy()\n",
    "#print 'Shape of df12 : {}'.format(df12.shape)\n",
    "#df13 = data13.drop(['#rating'], axis=1).copy()\n",
    "#print 'Shape of df13 : {}'.format(df13.shape)\n",
    "#df23 = data23.drop(['#rating'], axis=1).copy()\n",
    "#print 'Shape of df23 : {}'.format(df23.shape)\n",
    "\n",
    "print 'done in %0.3fs.' % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need about ~30 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#t0 = time()\n",
    "#(data1, label1, users1, items1) = build_data(df1)\n",
    "#(data2, label2, users2, items2) = build_data(df2)\n",
    "#(data3, label3, users3, items3) = build_data(df3)\n",
    "#print 'done in %0.3fs.' % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need about ~8 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#t0 = time()\n",
    "#df1 = pd.DataFrame(data1).join(pd.DataFrame(label1))\n",
    "#df1.set_axis(1, ['joke_id', 'user_id', 'rating'])\n",
    "#df2 = pd.DataFrame(data2).join(pd.DataFrame(label2))\n",
    "#df2.set_axis(1, ['joke_id', 'user_id', 'rating'])\n",
    "#df3 = pd.DataFrame(data3).join(pd.DataFrame(label3))\n",
    "#df3.set_axis(1, ['joke_id', 'user_id', 'rating'])\n",
    "#print 'done in %0.3fs.' % (time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d1 = df1.copy()\n",
    "#d1['rating'] = np.floor(d1['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1.hist(bins=20, grid=False, column='rating')\n",
    "plt.xlim(-10, 10)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('#Rating')\n",
    "plt.title('Distribution of ratings')\n",
    "plt.show()\n",
    "\n",
    "d1.hist(bins=20, grid=False, column='rating')\n",
    "plt.xlim(-10, 10)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('#Rating')\n",
    "plt.title('Distribution of ratings')\n",
    "plt.show()\n",
    "\n",
    "d1.groupby('user_id')['rating'].count().hist(bins=20, grid=False)\n",
    "plt.xlabel('Number of ratings')\n",
    "plt.ylabel('#Users')\n",
    "plt.title('Number of users with a given rating density')\n",
    "plt.show()\n",
    "\n",
    "d1.groupby('user_id')['rating'].mean().hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_df1 = pd.DataFrame()\n",
    "for index, joke in df1.iteritems():\n",
    "    normalized_df1 = pd.concat([normalized_df1, np.divide(np.subtract(joke,np.mean(joke)),np.std(joke))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "B = df1.as_matrix()\n",
    "W = np.ones(df1.shape)\n",
    "Psi, Phi = incremental_als(B, W, 2)\n",
    "print loss(Psi, Phi, B, W)\n",
    "print Psi.dot(Phi)\n",
    "\n",
    "print 'done in %0.3fs.' % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need about ~30 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "(train_data, y_train, train_users, train_items) = build_data(df1)\n",
    "(test_data, y_test, test_users, test_items) = build_data(df2)\n",
    "print 'done in %0.3fs.' % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need about ~30 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "v = DictVectorizer()\n",
    "X_train = v.fit_transform(train_data) # X_train.shape is the number of tuple (user_i, joke_j, rating_ij != 99)\n",
    "X_test = v.transform(test_data) # X_test.shape is the number of tuple (user_i, joke_j, rating_ij != 99)\n",
    "print 'done in %0.3fs.' % (time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = 10 - y_train\n",
    "y_test = 10 - y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need about ~16 min for 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "fm = pylibfm.FM(num_factors=10, num_iter=5, verbose=True, task=\"regression\", initial_learning_rate=0.001, learning_rate_schedule=\"optimal\")\n",
    "fm.fit(X_train,y_train)\n",
    "print 'done in %0.3fs.' % (time() - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
